{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASLib Overview\n",
    "NASLib is framework that was built in order to facilitate neural architecture search (NAS) research and development. Please refer to the slides and to the NAS survey paper for more details. In a high-level NASLib consists of 4 main building blocks which (can) interact with each other:\n",
    "\n",
    "- search spaces (cell search space, hierarchical, ...)\n",
    "- optimizers (one-shot/weight-sharing optimizers, black-box optimizers)\n",
    "- predictors (performance estimators that given an architecture as input, output its performance)\n",
    "- evaluators (run the architecture search loop and the final network training pipeline)\n",
    "\n",
    "Please follow the installation and setup guide in the main README of the repository so that you are able to ```import naslib``` without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Spaces in NASLib\n",
    "The search space representation is of primary importance for NASLib in ensuring that optimizers and search spaces can be combined in a variety of ways. The predominant way of representing NAS search spaces is the directed acyclic graph (DAG). In order to accomplish the aforementioned functionality of search spaces and computational graphs, we inherit in our basic graph classes from both *PyTorch* and *NetworkX*. The latter is a well-maintained and tested Python package for graph creation and manipulation, where node and edge attributes can be arbitrary Python objects. This framework allows us to represent multiple layers of graphs on top of the computational graph, allowing us to treat nodes and edges both as primitive operations (e.g. convolution), but also nested graph-structures such as a DARTS cell, to create e.g. macro architectures of stacked cells. NetworkX allows to easily construct the search space via ```add_node, remove_node, add_edge, remove_edge```, or traverse the topologically sorted graph in the forward pass of the PyTorch module using ```networkx.algorithms.dag.topological_sort```.\n",
    "\n",
    "## Case study: NASBench-201\n",
    "Case study: NAS-Bench-201\n",
    "The [NAS-Bench-201](https://arxiv.org/abs/2001.00326) is a tabular benchmark, i.e. a benchmark where you can simply query (already has been trained) the performance and other metrics of a specific architecture in the search space given that as an input. Its search space consists of a single normal cell which is replicated multiple times in a macro architecture interleaved by manually defined resnet-like reduction cells. The cell topology is fixed in the cell and consists of:\n",
    "\n",
    "1 input, 2 intermediate and 1 output node;\n",
    "a summation operation on each of the intermediate and output nodes;\n",
    "5 operation choices in each of the edges connecting 2 nodes\n",
    "- 'none'\n",
    "- 'skip_connect'\n",
    "- 'nor_conv_1x1'\n",
    "- 'nor_conv_3x3'\n",
    "- 'avg_pool_3x3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "device: cpu\n",
      "device: cuda:0\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Update function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
      "Update function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Update function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n"
     ]
    }
   ],
   "source": [
    "    from naslib.search_spaces import NasBench201SearchSpace as NB201\n",
    "\n",
    "    # instantiate the search space object\n",
    "    search_space = NB201()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black-box optimizers in NASLib\n",
    "After learning about the search space object, now we can add the other component of NAS: the NAS optimizer which you will use to search for an optimal architecture in that search space. A search space graph object can be interpreted in different ways depending on the type of optimizer being used. Here is the point where the search space and optimizer objects interact by parsing information from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some utilities and parse the configuration file\n",
    "import logging\n",
    "\n",
    "from naslib.utils import utils, setup_logger, get_dataset_api\n",
    "\n",
    "# This will read the parameters from the default yaml configuration file, which in this \n",
    "# case is located in NASLib/naslib/benchmarks/nas_predictors/discrete_config.yaml.\n",
    "# You do not have to change this but you can play around with its parameters.\n",
    "config = utils.get_config_from_args(config_type=\"nas_predictor\")\n",
    "utils.set_seed(config.seed)\n",
    "utils.log_args(config)\n",
    "\n",
    "logger = setup_logger(config.save + \"/log.log\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naslib.optimizers import RegularizedEvolution as RE\n",
    "\n",
    "# instantiate the optimizer object using the configuration file parameters\n",
    "optimizer = RE(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing the configuration file and instantiating the NAS optimizer and search space objects, we have to adapt the search space based on the optimizer type. A black-box optimizer such as Random Search will sample single architectures using the sample_random_architecture method of the search space object (e.g. by sampling one operation at each graph edge from the operation choices in NAS-Bench-201) throughout the optimization process. On the other hand most one-shot optimizers, such as DARTS, will interpret a set of operation choices on an edge as a MixedOp and assign an appropriate number of architectural weights (between 0 and 1, such that the sum is 1) to the outputs of each operation in order to obtain the continuous relaxation.\n",
    "\n",
    "Download the NAS-Bench-201 data from https://drive.google.com/file/d/17EBlTidimMaGrb3fE0APbljJl-ocgfs4/view?usp=sharing and place it in ```NASLib/naslib/data/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will load the NAS-Bench-201 data (architectures and their accuracy, runtime, etc).\n",
    "dataset_api = get_dataset_api(config.search_space, config.dataset)\n",
    "\n",
    "# adapt the search space to the optimizer type\n",
    "optimizer.adapt_search_space(search_space, dataset_api=dataset_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the only step left is to run the search. For this we will use the ```Trainer``` object in NASLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/15 22:07:24 nl.defaults.trainer]: \u001b[0mparam size = 0.000000MB\n"
     ]
    }
   ],
   "source": [
    "from naslib.defaults.trainer import Trainer\n",
    "\n",
    "# since the optimizer has parsed the information of the search space, we do not need to pass the search\n",
    "# space object to the trainer when instantiating it.\n",
    "trainer = Trainer(optimizer, config, lightweight_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/15 22:07:32 nl.defaults.trainer]: \u001b[0mStart training\n",
      "\u001b[32m[11/15 22:07:32 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/15 22:07:32 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/15 22:07:32 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
      "\u001b[32m[11/15 22:07:32 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 1\n",
      "\u001b[32m[11/15 22:07:32 nl.defaults.trainer]: \u001b[0mEpoch 0, Anytime results: {'cifar10-valid': {'train_losses': [1.8448507370758056, 1.4482701889801026, 1.2033821305465697, 1.0578395529174804, 0.9672217133712768, 0.9047809088516235, 0.8624664130783081, 0.8102531592178345, 0.7739742643928528, 0.7372287948799133, 0.7112728433036805, 0.6888134226989746, 0.656216381034851, 0.6430078009033203, 0.6244383986091614, 0.6145832547187805, 0.6030634831237793, 0.5876889658927917, 0.5698046956253052, 0.5695302256393433, 0.554458900642395, 0.5459589125442504, 0.5411161581134796, 0.5333880980491639, 0.5308071648788452, 0.5182912595939636, 0.5067689722251892, 0.5090348043823242, 0.5008928175354004, 0.494336733341217, 0.48755508690834043, 0.4870351034450531, 0.47154926067352293, 0.46818636558532717, 0.47359346885681153, 0.46702277557373045, 0.4662165118598938, 0.45169790756225586, 0.45394309759140017, 0.456775634803772, 0.45164650003433227, 0.44262144105911255, 0.44499721566200257, 0.43674357760429383, 0.4420267002296448, 0.4304736201286316, 0.42957851523399354, 0.43264751896858217, 0.4257934337615967, 0.4101024646663666, 0.4154289669799805, 0.42113240156173704, 0.4094417950820923, 0.4134802430534363, 0.4091940540599823, 0.4122182574939728, 0.4115633404254913, 0.4030672093963623, 0.39507799922943115, 0.4003704046821594, 0.3928971780872345, 0.396553594083786, 0.3977557871437073, 0.3854027569770813, 0.3823824584579468, 0.38050017569541933, 0.3752838945770264, 0.37899846725463865, 0.3752014892959595, 0.3751531733417511, 0.36648216248512266, 0.3644431753444672, 0.36426999655723574, 0.35916867698669436, 0.35515822687149046, 0.3565191607570648, 0.35268481311798094, 0.35011142058372496, 0.34423923651695254, 0.3528702948474884, 0.3447775728416443, 0.3423595958518982, 0.3395831126022339, 0.34585771060943604, 0.3342513404083252, 0.3306532184314728, 0.3283918639945984, 0.3314874352359772, 0.3221637986755371, 0.32489469586372377, 0.3252556687259674, 0.31230030658721925, 0.3127502564048767, 0.31675462012290956, 0.31130378761291505, 0.3057819596004486, 0.3053012943649292, 0.3010599161529541, 0.2996526253032684, 0.2992336745834351, 0.2884350463676453, 0.2865609113025665, 0.28132630244255064, 0.28003909135818483, 0.28774748054504395, 0.2754191650009155, 0.26997798232555387, 0.2701131276702881, 0.2731613219261169, 0.25842731949806214, 0.2743152234840393, 0.25166636305332185, 0.2605846247959137, 0.2568732134246826, 0.24841437654018403, 0.25032729298591616, 0.24700260801315307, 0.24014288452148438, 0.24011426775455474, 0.2390379490661621, 0.2361330591392517, 0.22773544178962707, 0.22617394111633302, 0.22070472088336945, 0.2194723853111267, 0.21508503527641296, 0.2100883407974243, 0.2057371665716171, 0.21080677266120912, 0.199403408203125, 0.19781574751615524, 0.19645344160079956, 0.18979232310295105, 0.1832240813112259, 0.172343241481781, 0.18108056477069856, 0.1775823163986206, 0.17507707698345185, 0.17027060317993165, 0.15823403495311736, 0.15549486690998077, 0.16175397060871124, 0.15709195439338683, 0.15652600651741028, 0.14525141629219054, 0.14846087557792664, 0.13772305532455445, 0.13558124988555909, 0.1285605062699318, 0.12078526411533355, 0.12623922095775605, 0.1195246507883072, 0.11492507241725922, 0.11179363048076629, 0.10424862047195435, 0.10459133654117585, 0.09855064447879791, 0.0921802650976181, 0.09697685758113861, 0.09244022409439087, 0.0865891903758049, 0.0884512786579132, 0.08186572254180909, 0.07770891021251679, 0.07426049928426742, 0.07172041123390198, 0.0691389311504364, 0.06451959282875061, 0.06391362716674805, 0.05929520119190216, 0.05811351816177368, 0.05625349636316299, 0.050188477237224575, 0.04622446121931076, 0.04587424839496613, 0.044028326723575595, 0.04425011084914207, 0.04348345280766487, 0.03986836673617363, 0.040377862417697905, 0.038385481110811234, 0.037979538447856905, 0.03753679785370827, 0.036642113519906996, 0.03629445140123367, 0.03532547743678093, 0.03358718937218189, 0.031670245931148526, 0.03018266411066055, 0.032052682435214516, 0.03077923007130623, 0.030920049061775208, 0.031409279404878615, 0.030420279346108435, 0.029118127703666688, 0.029781713322997092, 0.028883044391870498, 0.027961830661296844, 0.028647136284708975, 0.0312692619907856], 'eval_losses': [2.209690679244995, 1.5129467441177369, 1.599537881011963, 1.2754242917251586, 1.730937248878479, 1.3505103066253663, 1.1541910372924804, 1.150219281387329, 1.0220546703529358, 0.8652171747207642, 1.0771001834106446, 0.8424846367073059, 0.8633891479301453, 0.9101027658843994, 0.8882408099365234, 1.0354884182357789, 1.3376404821777343, 0.9326530316925049, 0.7665493152999878, 0.859800560760498, 1.1245292535209657, 1.0600102312088013, 0.7990478475379944, 0.7498183848190307, 0.7299501764297486, 0.8044170612335205, 0.9515656544494628, 0.8462035442352295, 0.9432493122291565, 0.6675951377868652, 0.7750381970787048, 0.8115145199012757, 0.6206709576225281, 0.7920350147819519, 0.8469941552352905, 0.7311203950500488, 2.247177856216431, 0.6979003336906433, 0.6961192506408691, 0.6735729881477356, 0.7741873324775695, 0.7777585388946533, 1.0438526329803466, 0.8736070625686646, 0.8565206823348999, 0.6901302116394042, 1.0089228301620483, 0.8771494263839722, 0.669878960571289, 0.9540154295349121, 0.7612296549797058, 0.8704987261199951, 0.688098923778534, 0.6993360045814514, 0.85451347032547, 0.8755119901657105, 0.6590913739585876, 0.7531925726318359, 0.738662105178833, 0.6517695015335083, 0.8611955827903748, 0.7724845533370972, 0.9831713185882568, 0.9505315917205811, 0.9252107932853699, 0.7277613755607605, 0.7719783405303955, 0.6099849526977539, 0.691449617023468, 0.720807094745636, 0.7606287134361267, 0.6450161028575897, 0.761694979686737, 0.5734113904380799, 0.7988319816207886, 0.6690421557617188, 0.6640055588722229, 0.7439274785995483, 0.647011937828064, 0.5990413676834107, 0.6923453042602539, 0.7186204867744446, 0.9595110310554504, 0.9938669442749023, 0.6702527696609497, 0.6180492558670044, 0.5839886853027344, 0.8032584553337098, 0.655567728176117, 0.6180389262199402, 0.7129864447593689, 0.5784852397727966, 0.9641488073348999, 0.6812978915214538, 0.6132553704833984, 0.8298053520202636, 0.804363727607727, 0.9019162292480469, 0.6141478131103516, 0.7637934473991395, 0.6588743051910401, 0.558283286037445, 0.6525457180023193, 0.7767801170539856, 0.6866563551521301, 0.6368999343299866, 0.6644655293273926, 0.5897802717208862, 0.5903786916923522, 0.5451140657043457, 0.6844050256347656, 0.6208961967468262, 0.6552243239212037, 0.6315489993667602, 0.6612986213302612, 0.6289024370479583, 0.5904618960762024, 0.5997077075958251, 0.5758347883605957, 0.7374957712936402, 0.5483319651985168, 0.6587945885467529, 0.5795303936386108, 0.5356696648597717, 0.622698008594513, 0.5967081747055054, 0.5903849782180786, 0.6497241812324523, 0.5725827060031891, 0.7209326259422302, 0.6210244026947022, 0.8097693787765503, 0.5948483721923828, 0.5404038514709473, 0.5651225442600251, 0.5857487120628357, 0.5926198264884949, 0.5616954737663269, 0.5346550869750977, 0.574935476436615, 0.5361388505554199, 0.5735026721954346, 0.6317190200042725, 0.5559490221405029, 0.5370168285942077, 0.5325959188079834, 0.5073195394897461, 0.5407564886188507, 0.5765104836273194, 0.6103313018226624, 0.561634503288269, 0.5625329159450531, 0.6277793484497071, 0.5663630598831176, 0.5869978824615478, 0.5239125629425049, 0.5904363479614257, 0.5799066771030426, 0.6219138694381714, 0.5896515998840332, 0.5304634125137329, 0.6115607666397095, 0.5642601796531678, 0.5109646224021912, 0.5375712536048889, 0.556248275384903, 0.5420360972595215, 0.5457275827217102, 0.5471574363708496, 0.5604304305362702, 0.5316952635574341, 0.5460505648612977, 0.5228192879295349, 0.5252304932594299, 0.5483994983100892, 0.5263834494686127, 0.5348044725608826, 0.5331913327884674, 0.5304804553031921, 0.5306942390727997, 0.522882447757721, 0.5296195610618591, 0.5198208953094482, 0.5269271646976471, 0.5307266926002503, 0.5333465632820129, 0.5221058327293396, 0.5286840767097473, 0.5244711859893799, 0.521272205619812, 0.5212426615142822, 0.5209880563926697, 0.5256801933860779, 0.5235021261978149, 0.5234351405525207, 0.5243646690177918, 0.5238478588581085, 0.5215264282608032, 0.5243822324371338, 0.5275063211822509, 0.5482983102321625], 'train_acc1es': [29.967999992675782, 45.612, 56.15200000610351, 62.075999982910155, 65.00399998779297, 67.56799998779297, 69.42399999511719, 71.25199997314454, 72.63600001464843, 74.33200000732423, 74.9880000048828, 75.92400000976562, 77.07999998779297, 77.07200001220703, 77.97999998779297, 78.37199998046874, 78.82800001220703, 79.66399999267578, 79.99999997070313, 80.03599998535157, 80.57599997070312, 81.03199998291015, 81.14799997558593, 81.32799998535157, 81.66800000976562, 81.97199997314453, 82.68000001708984, 82.04799998046875, 82.48399998291016, 82.67199998535156, 83.36399997802734, 82.87199997802735, 83.69600001953125, 83.73599998046875, 83.34399999023438, 83.58400001220703, 83.55600001708984, 84.27999997314453, 84.19200001464844, 84.19600000488282, 84.27999998291016, 84.48400001708984, 84.48800001220702, 84.40399998046875, 84.73199997558594, 85.07199997802735, 85.04, 84.74799997070312, 85.00400000976562, 85.56000000732422, 85.58800000732423, 85.23600000488281, 85.84399997070312, 85.7200000048828, 85.81599999755859, 85.75200001220703, 85.67200001464843, 85.81200000488282, 86.29200000488281, 85.99600001708984, 86.21600000976562, 86.15600001708984, 86.08400001220703, 86.69199999511719, 86.94400001220703, 86.848, 86.84000000732422, 86.76399998535156, 86.74799999023438, 86.79599999023438, 87.16000001464843, 87.32399999511719, 87.31599999755859, 87.60399999511719, 87.55599998779297, 87.65199998779296, 87.67199998291015, 87.67599997802735, 87.77600000488282, 87.7600000024414, 87.94399999023437, 88.14000000976563, 87.996, 87.92399998535156, 88.36399998779297, 88.52799999267579, 88.49199998779297, 88.37199997558594, 88.69199999267578, 88.66400000488281, 88.54000001220703, 88.89599999023437, 89.39600001220703, 88.83999998779296, 89.14399998535156, 89.2960000024414, 89.25199999023438, 89.46799999511718, 89.51600000732422, 89.47999998779297, 89.98799999023437, 89.98399999511719, 90.11599997802735, 90.02399997802735, 90.01599999023438, 90.3800000024414, 90.70799997070313, 90.5920000048828, 90.24399998535156, 90.9079999951172, 90.37600000488281, 91.28799998046875, 90.90000000488281, 90.88, 91.29999998779297, 91.18799998779296, 91.272, 91.62799999755859, 91.56399998291016, 91.73199998291015, 91.75999998291016, 92.03599997558594, 92.04, 92.33199998046875, 92.36800001220703, 92.39999999023438, 92.53599998046874, 92.82000001953125, 92.60799998046875, 92.93999997070313, 93.04800001464844, 93.10399998535156, 93.47599997314452, 93.62400001708984, 94.03200001220704, 93.57999997802735, 93.64399997314453, 93.89200001953125, 94.11600001464844, 94.59199997070313, 94.56800001953125, 94.38799997802734, 94.52799997802734, 94.51199997070313, 95.01999997802734, 94.87199997558594, 95.18400001953125, 95.25999997070312, 95.51199997314453, 95.88000001464843, 95.64800001953125, 95.75199997314454, 96.06000001953124, 96.09199997314452, 96.56800001464843, 96.46800000976563, 96.76000001220703, 96.96400001708984, 96.83600001220704, 96.88399997314453, 97.22400000732422, 96.99600001708984, 97.37600000976562, 97.55200001220703, 97.60800000976562, 97.68000001220703, 97.8, 97.98400000488282, 98.02000000732421, 98.17200000732421, 98.34000000732422, 98.31200000976563, 98.55200001708984, 98.73200000976563, 98.64000001220703, 98.75200000976562, 98.76400001220703, 98.7640000024414, 98.92, 98.94, 98.92400000488281, 99.00000000244141, 98.93200000488281, 99.08400000488281, 99.04400001220704, 99.08000000244141, 99.12, 99.18400000488282, 99.33600000244141, 99.244, 99.2720000024414, 99.2960000024414, 99.23600000488281, 99.308, 99.33200000488281, 99.3600000048828, 99.352, 99.3560000024414, 99.36, 99.2240000024414], 'eval_acc1es': [24.21600000488281, 45.967999986572266, 47.92799998657227, 55.26800000366211, 48.019999993896484, 57.6200000012207, 62.14399999023438, 62.27199998046875, 65.71599997558594, 70.63599997070312, 63.04399999511719, 71.04800001953124, 72.0960000024414, 69.78399997558594, 71.04799999511718, 65.80399999511718, 63.32400000854492, 68.77599998291015, 75.12800001953126, 72.8999999975586, 67.52399997314453, 67.94399997070313, 73.53999999511718, 74.49200000976562, 75.25599998046874, 72.88400001220703, 71.63200001953125, 73.75200000732421, 71.06000000732422, 78.34399998779297, 74.2200000048828, 73.4359999975586, 79.77999998779296, 74.80400001953124, 74.36400000488281, 76.28000001220703, 53.971999986572264, 77.39999998779297, 77.54799997802735, 78.16399997558594, 76.06800000244141, 75.06000000732422, 70.12399998291015, 74.16800001220703, 75.19200001464844, 78.09599999511718, 70.05200001708984, 71.66399997314453, 77.45999999023438, 71.4239999975586, 77.80399997802735, 73.62800000244141, 78.54800001708985, 78.45199998291015, 75.372, 73.54000001464844, 78.23999999267578, 76.92800001464843, 77.47199998291016, 78.48799999755859, 73.42000001708985, 76.45999999267578, 72.94400000732422, 72.04800001708985, 74.436, 78.24799997558594, 76.81600000976563, 80.31199997802734, 78.94799997802734, 78.0920000048828, 77.39999999511718, 79.38000001953125, 76.904, 81.78799997314454, 77.36799997802734, 78.96800000732422, 79.14399998535156, 78.86399998779297, 79.88399999267578, 81.65200001464844, 77.91999999267578, 77.57199999267579, 74.188, 73.16400001953124, 79.96799998291016, 80.444, 81.02800000488281, 77.428, 80.27200001953125, 80.53599997314453, 78.62399997802734, 81.37200001464844, 72.2719999975586, 79.83599997802735, 80.64799997802734, 77.90399998779297, 77.27999997070313, 76.02799999267579, 81.65999999023437, 78.02400000488281, 80.53599997070313, 82.04000001708984, 81.16800001220703, 78.59200000976563, 78.77999997802735, 81.00400001708984, 80.61599997558594, 82.33200001464844, 82.21599999267578, 83.86000001953126, 79.87999997558593, 82.08800001708984, 81.00799997802734, 81.63599997558593, 80.30800001953125, 81.71600000976562, 82.15200000732422, 82.03200001708984, 82.86799997802734, 78.20799999511719, 83.69199997314453, 81.68400001464843, 82.26000001708984, 83.84399998291016, 82.42800000488282, 82.33600001220704, 82.13600001464843, 81.85600001953125, 83.47599999023437, 80.41599997314454, 82.90399998779297, 78.83200000976562, 82.70400001464844, 84.33599997314452, 84.4800000024414, 83.28399997314453, 83.79999997314454, 83.99199998046875, 84.48000001464844, 83.74800001953125, 84.59999999511719, 84.00800001220703, 82.56000001953124, 84.10000001953125, 84.37999999511719, 84.95600000732422, 85.75599999023437, 85.32799999267579, 84.7320000048828, 84.13200000976562, 84.83200000732423, 85.16000000976562, 84.21200001953125, 85.04399997314454, 84.96000000488282, 86.05200000732422, 84.88800001953125, 85.04399998779297, 84.58399998535157, 85.03600001464844, 85.60400000732422, 84.73999997314453, 86.0359999951172, 86.96799999023438, 86.45200000244141, 86.356, 86.23200001464843, 86.45200001953125, 86.39600000488281, 86.236, 86.66400000244141, 86.78800000732421, 86.91599999267578, 87.06000001220703, 86.98400001220703, 87.26799999023437, 87.05599999755859, 87.12400000732421, 86.976, 87.17200000732421, 87.4599999975586, 87.32799997558594, 87.29599999511719, 87.35999999511719, 87.2919999951172, 87.18800000976563, 87.44800000976562, 87.30399997070313, 87.41199999023438, 87.46400000244141, 87.432, 87.34799998779297, 87.43200000488281, 87.41599999267578, 87.38799998779297, 87.43600000976562, 87.4999999951172, 87.46400001220704, 87.45599999755859, 87.38800000732422, 86.77], 'cost_info': {'flops': 15.64737, 'params': 0.129306, 'latency': 0.01448613718936318, 'train_time': 7.5402006308237715}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/15 22:07:32 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy (top1, top5): 99.22400, 0.00000, Validation accuracy: 86.77000, 0.00000\n",
      "\u001b[32m[11/15 22:07:32 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:33 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy (top1, top5): 99.22400, 0.00000, Validation accuracy: 86.77000, 0.00000\n",
      "\u001b[32m[11/15 22:07:33 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:33 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy (top1, top5): 99.22400, 0.00000, Validation accuracy: 86.77000, 0.00000\n",
      "\u001b[32m[11/15 22:07:33 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:33 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy (top1, top5): 99.96000, 0.00000, Validation accuracy: 89.47000, 0.00000\n",
      "\u001b[32m[11/15 22:07:33 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:33 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy (top1, top5): 99.96000, 0.00000, Validation accuracy: 89.47000, 0.00000\n",
      "\u001b[32m[11/15 22:07:33 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:33 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 6\n",
      "\u001b[32m[11/15 22:07:33 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy (top1, top5): 99.96000, 0.00000, Validation accuracy: 89.47000, 0.00000\n",
      "\u001b[32m[11/15 22:07:33 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:34 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy (top1, top5): 99.96000, 0.00000, Validation accuracy: 89.47000, 0.00000\n",
      "\u001b[32m[11/15 22:07:34 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:34 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy (top1, top5): 99.96000, 0.00000, Validation accuracy: 89.47000, 0.00000\n",
      "\u001b[32m[11/15 22:07:34 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:34 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy (top1, top5): 99.96000, 0.00000, Validation accuracy: 89.47000, 0.00000\n",
      "\u001b[32m[11/15 22:07:34 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:34 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:34 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:34 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 11\n",
      "\u001b[32m[11/15 22:07:34 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:34 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:35 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:35 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:35 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:35 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:35 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:35 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:35 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:35 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:35 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:35 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:36 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 17\n",
      "\u001b[32m[11/15 22:07:36 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:36 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:36 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:36 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:36 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:36 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:36 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:36 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:36 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:36 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:36 nl.defaults.trainer]: \u001b[0mEpoch 21 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:36 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:36 nl.defaults.trainer]: \u001b[0mEpoch 22 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:36 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:37 nl.defaults.trainer]: \u001b[0mEpoch 23 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:37 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:37 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 25\n",
      "\u001b[32m[11/15 22:07:37 nl.defaults.trainer]: \u001b[0mEpoch 24 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:37 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:37 nl.defaults.trainer]: \u001b[0mEpoch 25 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:37 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:37 nl.defaults.trainer]: \u001b[0mEpoch 26, Anytime results: {'cifar10-valid': {'train_losses': [1.9414194129562379, 1.657240117149353, 1.488989450416565, 1.3623449864959716, 1.2205402016830444, 1.082631254825592, 0.9670495380592347, 0.8688206433296204, 0.8056281983566285, 0.7579943205451966, 0.7179952316284179, 0.6761367119979859, 0.6415317040252686, 0.6234779255867005, 0.5991091899490356, 0.5729377473640442, 0.5708416128349304, 0.5455854043006897, 0.5326134916305542, 0.5165965073204041, 0.5094373629760742, 0.49235358980178834, 0.49251827865600584, 0.47933861349105833, 0.45774403624534604, 0.46455267183303833, 0.45366026344299315, 0.43875825168609617, 0.4434005892276764, 0.42718110469818116, 0.42950413142204286, 0.40737262907981875, 0.4065498887825012, 0.4044119661808014, 0.39249164374351503, 0.39140214116096494, 0.39121461182594297, 0.3837351051712036, 0.370652527179718, 0.37717686605453493, 0.37305590203285216, 0.36668004512786867, 0.36296121942520143, 0.3499062122344971, 0.35427993593215945, 0.3443461538696289, 0.3408590573883057, 0.33930111249923706, 0.3391232724952698, 0.329179435338974, 0.3198073943138123, 0.3209088639163971, 0.3306076374721527, 0.31250784534454346, 0.3093562066888809, 0.30507897426605224, 0.3119826993083954, 0.29785251916885375, 0.29939648101806643, 0.29402633812904355, 0.29093067388534544, 0.28190645393371583, 0.27946470293045045, 0.2787441945934296, 0.2731068988418579, 0.27461747042655943, 0.2766466921138763, 0.2665010407066345, 0.27075398522377014, 0.2626846249866486, 0.26021284505844117, 0.26315453797340393, 0.24608466579914093, 0.24123775084972382, 0.24425012459754944, 0.24113248934268952, 0.23636754945755004, 0.24038526586532594, 0.233247367811203, 0.23366640744686126, 0.23193993731975557, 0.2250266859292984, 0.22544129577159883, 0.21964467310905456, 0.2170447670888901, 0.21635848505973815, 0.22136135118961334, 0.20411008838176728, 0.19769249735355376, 0.2064019425392151, 0.19757946559906006, 0.20970634586334228, 0.19282142052650453, 0.1889273459291458, 0.19466492082595827, 0.18919842041969298, 0.1739389295244217, 0.17787426261901856, 0.17951737102508544, 0.1669521140909195, 0.1678585489845276, 0.173347176322937, 0.15713094361782073, 0.1659370524263382, 0.1615353871703148, 0.14725980876922606, 0.15892274024009703, 0.15253857725143433, 0.1446617264509201, 0.1457327261352539, 0.13453150416851042, 0.14321203130722046, 0.1307672835969925, 0.13618171226501466, 0.12942253145694732, 0.12731492314338685, 0.12524001580238342, 0.12208214060783386, 0.11534539397478104, 0.1142642702627182, 0.11611287380695343, 0.10274074279785156, 0.10950569625854492, 0.10738655640125275, 0.10042683569669723, 0.09471996492147446, 0.09056463571310043, 0.08995016838788986, 0.07872507861375809, 0.0878951308298111, 0.08327981725215912, 0.08014259706497193, 0.08338454184532165, 0.0728661936211586, 0.06623821694850922, 0.06853685590028763, 0.06154714248895645, 0.06271277683615685, 0.06220841185450554, 0.05390985393762589, 0.05403131470680237, 0.04911735908269882, 0.04841681674957275, 0.0502313981628418, 0.036987318425774574, 0.042752450956106185, 0.04285485763788223, 0.042366617243289946, 0.0395532922065258, 0.03242382596731186, 0.0377573931324482, 0.028042214423418046, 0.025187211338281633, 0.024161562922000886, 0.02633243676304817, 0.02337815177321434, 0.02134722484469414, 0.017784469883441924, 0.016566408001184463, 0.017759357714653014, 0.01487809175759554, 0.01025239658087492, 0.009191859903633595, 0.008857184725031256, 0.011300552709102631, 0.009664696311354637, 0.009330228491276503, 0.007244278211295605, 0.007044780759215355, 0.005456825104653835, 0.004707740907743573, 0.004722688645794988, 0.0037426230388879775, 0.003992714128345251, 0.0038098787494748832, 0.0036616202449798584, 0.003534279789924622, 0.0036596186258643867, 0.0032108434757590295, 0.003461337421834469, 0.003011767282858491, 0.002834831551834941, 0.0025036096378415825, 0.0028239959992468356, 0.002214495583176613, 0.0025398499299585818, 0.0023162052738666533, 0.002165452403947711, 0.002590071372836828, 0.0021677252575010063, 0.0026829876754060388, 0.0025115097737312317, 0.00255562937758863, 0.0018036418725550175, 0.0022889952655136586, 0.002521195917129517, 0.0021487063791975377, 0.0020214460181072354, 0.002016662092395127, 0.0021982241056859495], 'eval_losses': [1.968733893890381, 1.7749035670471192, 1.5148222987747193, 1.4004682809066773, 1.9099578909301758, 1.5363087120437622, 1.1762576742172242, 0.9220611395835876, 1.144130904159546, 1.294849601135254, 1.1890775186538696, 0.8525269490051269, 0.8104005944061279, 0.902269907951355, 0.7607875191879272, 0.7596632920646668, 0.7484353617858887, 0.6664900652313233, 0.7674936779212952, 0.8187258393859863, 1.0401746963882446, 0.6218926194381714, 0.9159524702072144, 0.8412337506484986, 1.0025667998886107, 0.7237920232391357, 0.759884122428894, 0.7561571546554565, 0.7713617223167419, 0.7134555696296692, 0.6389007352638245, 0.850425811958313, 0.8749064930534363, 0.7748780924987793, 1.1082467802047729, 0.7418988953399658, 0.7488931846618653, 0.8062931499290467, 1.1283457273864745, 0.8704337614059449, 0.8012056392288208, 1.0564893614768982, 0.6741657467842102, 0.6478140713882446, 0.8764697950553894, 0.9573751627731323, 0.7199712957572937, 0.7298770334243775, 0.5996048046684265, 0.6101030858421326, 0.8170461796188354, 0.8001913031387329, 0.7400443310165405, 0.6743746350669861, 0.7907923915672302, 0.7772476996803284, 0.879078306350708, 0.6993397441482544, 0.7629481049728394, 0.7571919723129272, 0.7344261824798584, 0.7057700606155396, 0.967249628314972, 0.8070355750465393, 0.5823871754837037, 0.7563765495491028, 0.6629283151817322, 0.6257257188606262, 0.5799200154495239, 0.5758247982597351, 0.7082703718757629, 0.501734511604309, 0.6198463360023498, 1.3355056199264526, 0.7383631519317627, 0.6271685657882691, 0.597206157951355, 0.6051623519515992, 0.860330905418396, 0.5385485094451904, 0.6384402573776246, 0.6336773563766479, 0.898652379322052, 0.6210466105270386, 0.7261086989974975, 0.5403772002220154, 0.6186943788146972, 0.6510431205558777, 0.6246236246585846, 0.7051578102111816, 0.8187968163490296, 0.6380074934196472, 0.5551311587810517, 0.6099945841598511, 0.6471321570968628, 0.5830700452041626, 0.528852765455246, 0.6534181815719604, 0.5700202668571472, 0.577045350856781, 0.5436607713317871, 0.6537132048797607, 0.7464215605926514, 0.6263451830673218, 0.7182158648872375, 0.6571725856018067, 0.5741001466560364, 0.6043054015350342, 0.5665896384620667, 0.5176432662010193, 0.5720805895900727, 0.524158226032257, 0.6074511522293091, 0.5476491460227967, 0.6192447629547119, 0.5956588020324707, 0.7249159648513794, 0.5595117812538147, 0.5410520198440552, 0.5462071184921264, 0.5195467142009735, 0.5460605216598511, 0.5930256019973755, 0.6020939514446259, 0.5354613296699524, 0.664857619934082, 0.600053364391327, 0.580762954235077, 0.5924841109848022, 0.5630148586082458, 0.5543156016731262, 0.5493629027366638, 0.5582247809028625, 0.5353425050735474, 0.6486110217475891, 0.5971841025352478, 0.5122486492061615, 0.6277944529914856, 0.5316347860431672, 0.6129234597587585, 0.5447159000205993, 0.5456936353492737, 0.6412661179161072, 0.5639679259300232, 0.5452911881637573, 0.6396825536727905, 0.5117951903057099, 0.5719357240486145, 0.516720093574524, 0.5801180385494232, 0.5398574902915955, 0.5218429987144471, 0.5069131775283814, 0.5376834706306457, 0.5510293472862243, 0.5117803416872024, 0.5056479605770111, 0.6177353524780274, 0.5238969486999512, 0.5289664856147767, 0.5395895210266113, 0.5185777901554107, 0.521138453617096, 0.5149957319068909, 0.5167409827804565, 0.5167478098678588, 0.5217331728744506, 0.5305897522830963, 0.5019137520980835, 0.493610832490921, 0.5060894581794739, 0.49752404168128966, 0.4949512928390503, 0.49815265632629396, 0.5062598399925232, 0.5015283616828918, 0.49831454919815066, 0.5019300793552399, 0.494189912981987, 0.49573956468582153, 0.49230046558380125, 0.4903559283447266, 0.4886589603805542, 0.48330799112319944, 0.4840067990350723, 0.4879802134513855, 0.4881051842212677, 0.49045352033615114, 0.485833915348053, 0.4870227941036224, 0.48910427413940427, 0.4859510673904419, 0.49026022603988645, 0.4861213781356811, 0.48706778468132017, 0.4879261607170105, 0.48596753909111023, 0.48507183238983154, 0.4856188381385803, 0.4894196392250061, 0.5103639570236206], 'train_acc1es': [25.847999997558595, 37.876000007324215, 44.655999991455076, 49.96800000366211, 55.672, 61.11199997802734, 65.81999998535156, 69.38399997070313, 71.90399997314454, 73.536, 74.77599999267578, 76.19599998046876, 77.84799997070313, 78.47599999511719, 79.19199998779297, 80.27599997802734, 80.16399998779296, 81.13999997070313, 81.75999998046875, 82.03599997802735, 82.69999997314453, 83.01600001708984, 82.98400001708984, 83.66400000488281, 84.30400001464844, 83.85599997558593, 84.16800001220703, 84.82399997802735, 84.676, 85.22800001464844, 85.23999999267578, 86.1479999975586, 85.91200001953125, 86.1440000024414, 86.54000001953125, 86.54399998779297, 86.58800000976562, 86.51600001464844, 87.05200000488281, 86.82000001220703, 87.19199999511719, 87.40000000488281, 87.59200001220704, 87.91999999511718, 87.752, 88.13599997070312, 88.34800000732422, 88.3879999975586, 88.25199998779297, 88.71200000976563, 89.06000000244141, 88.7120000024414, 88.4000000024414, 89.28399999023438, 89.21599998046875, 89.36399999023438, 89.18, 89.6399999951172, 89.684, 89.91199999511718, 89.98, 90.30399999511718, 90.39600000244141, 90.29599998779297, 90.4159999975586, 90.42799999023437, 90.42399997558594, 90.79200000976563, 90.68399999023437, 90.96400000244141, 90.99600000488282, 91.05199999023438, 91.45999998779297, 91.68399998291015, 91.59199999755859, 91.67199997558593, 91.74799999267579, 91.72799998779297, 91.87199997558594, 91.99999997558594, 92.02399998046874, 92.17199998046875, 92.14800001953125, 92.45199997070313, 92.56800001953125, 92.59199998779297, 92.35999997558594, 92.97599997558594, 93.22399997314453, 92.88799999511718, 93.21999999023437, 92.58400001953125, 93.18799999023437, 93.50000001953126, 93.34799998046876, 93.34399998779297, 94.00799998291015, 93.71999998779297, 93.90799997558594, 94.21600001708984, 94.20799997314454, 93.91199997070312, 94.58000001953125, 94.49999997314453, 94.46000001708984, 95.01599997558594, 94.60399999023437, 94.73999997558593, 94.99600001953125, 94.97199998046875, 95.36400001464844, 95.01199997314453, 95.45599997070312, 95.31200000976563, 95.53999997558594, 95.73599997802734, 95.69200001953125, 95.86399997070312, 95.98400000976562, 96.11199997314453, 95.90399997314454, 96.44400001464844, 96.29200001953124, 96.31599997070313, 96.48800001464843, 96.87200000976563, 96.84800001708984, 96.87200001220702, 97.47200000976562, 96.97600000976563, 97.19199997802734, 97.18000001708984, 97.16000000976562, 97.46000001464844, 97.78399997558594, 97.72000000976563, 97.98400000488282, 97.89200000488282, 97.93600000976562, 98.24400000732422, 98.25600000732422, 98.36400000976562, 98.36400001220703, 98.33200000488281, 98.8080000024414, 98.58000000488282, 98.6040000048828, 98.54400000488282, 98.756, 98.98400001708984, 98.7360000024414, 99.12800000488281, 99.21600000488282, 99.23200000976563, 99.16, 99.2760000024414, 99.36, 99.48400000732421, 99.54400000244141, 99.45200000488282, 99.60400000244141, 99.756, 99.764, 99.76, 99.66400000244141, 99.744, 99.772, 99.8, 99.832, 99.88, 99.92, 99.904, 99.94000000244141, 99.936, 99.932, 99.932, 99.94400000244141, 99.928, 99.948, 99.932, 99.96, 99.96, 99.968, 99.948, 99.98800000244141, 99.968, 99.972, 99.972, 99.952, 99.988, 99.952, 99.9720000024414, 99.956, 99.996, 99.9720000024414, 99.952, 99.972, 99.98, 99.984, 99.972], 'eval_acc1es': [29.471999998168947, 32.48799998901367, 43.436000001220705, 49.595999990234375, 42.93199999633789, 49.292000006103514, 59.11199999755859, 68.41200001953125, 61.80800000854492, 59.52799998657227, 61.072, 71.03600001953124, 73.0520000024414, 69.7839999975586, 74.144, 74.01999998779297, 75.23199998046876, 77.68399999511719, 74.55200000488281, 74.00399998291016, 69.54399998779297, 78.94399999023437, 71.816, 73.54000000488281, 68.39999998779297, 76.24799999511718, 74.7, 75.91200000488281, 76.05199998779297, 77.7040000024414, 78.86399997070312, 74.2279999975586, 74.05199997558594, 76.29999999267578, 69.97600001220704, 77.20800000244141, 77.01599997802734, 76.0520000024414, 70.59200001953126, 74.00799997558593, 75.63199998291016, 70.0640000024414, 79.34399997314453, 79.62799997802735, 74.16399998779296, 72.27599998535156, 76.856, 77.90799998535157, 80.53200001708984, 80.53999997558594, 75.95199998779297, 76.81599998046875, 77.63600001220703, 79.46399998779297, 77.37599997558594, 77.10799998535157, 74.39599999023437, 78.83599999023437, 76.84799998535156, 79.02000001708984, 78.86399997070312, 79.17200001220704, 73.53200000488282, 76.84799998046876, 81.95199997314454, 77.1799999975586, 80.18399998291015, 81.14399997314453, 82.43199997558594, 82.33999997802735, 80.56399997558594, 84.36000000976563, 82.16399999023437, 68.57999997070313, 79.47599997314452, 81.74800001953125, 82.78000001708985, 82.1359999975586, 77.12000000732422, 83.99999997070313, 82.04400001953125, 81.91199998046875, 75.94799997314453, 82.33199998046875, 80.19999999267579, 83.51200000732422, 82.40399997314454, 80.62800000976563, 82.03999999511718, 80.62800001708985, 78.1680000024414, 82.19199997070312, 83.91200001220703, 82.51200000976563, 81.68399998779297, 83.70799997070313, 84.8879999975586, 82.12000001464844, 83.72400001220703, 83.73200001464843, 85.19200001708984, 82.16799997314453, 80.76399998291015, 83.25599998046874, 81.41599999511719, 83.07600001708984, 84.29600000488281, 83.98400000976562, 84.37599997802734, 86.004, 84.88400000732422, 85.66399997558594, 84.96799997070312, 85.05200001953125, 84.41599998046875, 84.61599997070313, 81.85999998046876, 85.19999998291016, 85.88000000976562, 85.77600001464843, 86.22, 85.77600001220704, 85.30799997314453, 84.87199999023437, 85.92000000732422, 83.84400000976562, 85.26800001220703, 85.80000000488282, 85.23199999755859, 85.92399999511719, 86.07999998779297, 86.36800001464844, 86.64400000976562, 86.75600000244141, 84.92800000732421, 85.38400000976563, 87.38399999267578, 85.388, 87.0, 85.86000001953126, 87.12400001220703, 87.29999998779297, 85.53599999755859, 87.15600001708984, 87.45600000976563, 85.73599997802734, 87.84799999267578, 87.07999999267578, 88.10399998291015, 87.12799999023437, 87.83600000732422, 88.2799999951172, 88.85999999267578, 88.25999998779297, 88.05599999511719, 88.92399998291016, 89.10799997802734, 87.32399999267578, 88.9079999951172, 88.76799998535157, 88.89600000976563, 89.26799998291015, 89.36799999267578, 89.2959999975586, 89.3640000024414, 89.19199998779297, 89.25999997802734, 89.32799998046875, 89.72400000732422, 89.73999999023438, 89.7039999975586, 89.85999998291015, 89.92799999267578, 89.91599999023437, 89.82399999755859, 89.85599998779297, 90.004, 89.93599997558594, 90.08800001953125, 89.92799998779297, 90.12400000976562, 90.03999998779297, 90.13599997314454, 90.19999998535157, 90.11999997314453, 90.08, 90.14799997802734, 90.12400000732421, 90.12799998291015, 90.17200000244141, 90.13199999267579, 90.1799999975586, 90.11199997070312, 90.17599998291016, 90.16399998535157, 90.18799998046875, 90.15599998046875, 90.1799999975586, 90.17999998779297, 90.14399999511718, 89.89], 'cost_info': {'flops': 153.27297, 'params': 1.073466, 'latency': 0.02210158109664917, 'train_time': 14.65835303068161}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/15 22:07:37 nl.defaults.trainer]: \u001b[0mEpoch 26 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:37 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:38 nl.defaults.trainer]: \u001b[0mEpoch 27 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:38 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:38 nl.defaults.trainer]: \u001b[0mEpoch 28 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:38 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
      "\u001b[32m[11/15 22:07:38 nl.defaults.trainer]: \u001b[0mEpoch 29 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:38 nl.defaults.trainer]: \u001b[0mEpoch 30 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:38 nl.defaults.trainer]: \u001b[0mEpoch 31 done. Train accuracy (top1, top5): 99.97200, 0.00000, Validation accuracy: 89.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:38 nl.defaults.trainer]: \u001b[0mEpoch 32 done. Train accuracy (top1, top5): 99.98000, 0.00000, Validation accuracy: 90.23000, 0.00000\n",
      "\u001b[32m[11/15 22:07:39 nl.defaults.trainer]: \u001b[0mEpoch 33 done. Train accuracy (top1, top5): 99.98000, 0.00000, Validation accuracy: 90.51000, 0.00000\n",
      "\u001b[32m[11/15 22:07:39 nl.defaults.trainer]: \u001b[0mEpoch 34 done. Train accuracy (top1, top5): 99.98000, 0.00000, Validation accuracy: 90.51000, 0.00000\n",
      "\u001b[32m[11/15 22:07:39 nl.defaults.trainer]: \u001b[0mEpoch 35 done. Train accuracy (top1, top5): 99.98000, 0.00000, Validation accuracy: 90.51000, 0.00000\n",
      "\u001b[32m[11/15 22:07:39 nl.defaults.trainer]: \u001b[0mEpoch 36 done. Train accuracy (top1, top5): 99.98000, 0.00000, Validation accuracy: 90.51000, 0.00000\n",
      "\u001b[32m[11/15 22:07:39 nl.defaults.trainer]: \u001b[0mEpoch 37 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.58000, 0.00000\n",
      "\u001b[32m[11/15 22:07:39 nl.defaults.trainer]: \u001b[0mEpoch 38 done. Train accuracy (top1, top5): 99.96800, 0.00000, Validation accuracy: 90.58000, 0.00000\n",
      "\u001b[32m[11/15 22:07:40 nl.defaults.trainer]: \u001b[0mEpoch 39 done. Train accuracy (top1, top5): 99.99200, 0.00000, Validation accuracy: 90.74000, 0.00000\n",
      "\u001b[32m[11/15 22:07:40 nl.defaults.trainer]: \u001b[0mEpoch 40 done. Train accuracy (top1, top5): 99.99200, 0.00000, Validation accuracy: 90.74000, 0.00000\n",
      "\u001b[32m[11/15 22:07:40 nl.defaults.trainer]: \u001b[0mEpoch 41 done. Train accuracy (top1, top5): 99.99200, 0.00000, Validation accuracy: 90.74000, 0.00000\n",
      "\u001b[32m[11/15 22:07:40 nl.defaults.trainer]: \u001b[0mEpoch 42 done. Train accuracy (top1, top5): 99.99200, 0.00000, Validation accuracy: 90.74000, 0.00000\n",
      "\u001b[32m[11/15 22:07:40 nl.defaults.trainer]: \u001b[0mEpoch 43 done. Train accuracy (top1, top5): 99.99200, 0.00000, Validation accuracy: 90.74000, 0.00000\n",
      "\u001b[32m[11/15 22:07:41 nl.defaults.trainer]: \u001b[0mEpoch 44 done. Train accuracy (top1, top5): 99.99200, 0.00000, Validation accuracy: 90.74000, 0.00000\n",
      "\u001b[32m[11/15 22:07:41 nl.defaults.trainer]: \u001b[0mEpoch 45 done. Train accuracy (top1, top5): 99.99200, 0.00000, Validation accuracy: 90.74000, 0.00000\n",
      "\u001b[32m[11/15 22:07:41 nl.defaults.trainer]: \u001b[0mEpoch 46 done. Train accuracy (top1, top5): 99.99200, 0.00000, Validation accuracy: 90.74000, 0.00000\n",
      "\u001b[32m[11/15 22:07:41 nl.defaults.trainer]: \u001b[0mEpoch 47 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 90.89000, 0.00000\n",
      "\u001b[32m[11/15 22:07:41 nl.defaults.trainer]: \u001b[0mEpoch 48 done. Train accuracy (top1, top5): 100.00000, 0.00000, Validation accuracy: 90.93000, 0.00000\n",
      "\u001b[32m[11/15 22:07:42 nl.defaults.trainer]: \u001b[0mEpoch 49 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:42 nl.defaults.trainer]: \u001b[0mEpoch 50 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:42 nl.defaults.trainer]: \u001b[0mEpoch 51 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:42 nl.defaults.trainer]: \u001b[0mEpoch 52 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:42 nl.defaults.trainer]: \u001b[0mEpoch 53 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:42 nl.defaults.trainer]: \u001b[0mEpoch 54 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:42 nl.defaults.trainer]: \u001b[0mEpoch 55 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:42 nl.defaults.trainer]: \u001b[0mEpoch 56 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:43 nl.defaults.trainer]: \u001b[0mEpoch 57, Anytime results: {'cifar10-valid': {'train_losses': [1.7928205490493774, 1.307014076385498, 1.0573206016921997, 0.9273022071456909, 0.8365537706375122, 0.7531461843490601, 0.6857421253395081, 0.6359179971885681, 0.6039436100959777, 0.5622060668182373, 0.5312232629776001, 0.5127179312515259, 0.49226710228919984, 0.47680529086112977, 0.45521620368003846, 0.4401175368309021, 0.43458957195281983, 0.4233137342643738, 0.4094787258434296, 0.3991302064228058, 0.3878906077003479, 0.3823627779960632, 0.37167100338935855, 0.3705644884109497, 0.3600892097663879, 0.35521268272399903, 0.3462184637737274, 0.3395050159740448, 0.33368458634376524, 0.32823342905044556, 0.32607923354148866, 0.3115603800868988, 0.3062793056964874, 0.3062149836921692, 0.31188280254364015, 0.2847984163856506, 0.2871338624382019, 0.2898564466190338, 0.2819711438179016, 0.2754035506153107, 0.27300779004096987, 0.26851205161094666, 0.2635558602046967, 0.27253141528129576, 0.2681609213066101, 0.2591433022689819, 0.25184848851203917, 0.2462521582698822, 0.24920288200378418, 0.24619078300476074, 0.23942322838783264, 0.23391515253067016, 0.23535483856678008, 0.23740831488609315, 0.22513909769058227, 0.22805173836708068, 0.224331509141922, 0.22185394792556762, 0.2110287701702118, 0.21622198316574096, 0.2094046536874771, 0.2065801433467865, 0.20865640159606932, 0.20254249185562134, 0.20277870695114136, 0.19328316625595093, 0.197144799785614, 0.19601931133270264, 0.1851720029449463, 0.18983122215747833, 0.1852110726547241, 0.18554520138263703, 0.180264482588768, 0.17638678524494172, 0.1744976638841629, 0.1702513282585144, 0.17366377560138702, 0.17342201046943664, 0.1658402132987976, 0.16579039964199066, 0.16067870686531066, 0.16337166574478149, 0.15072033410072327, 0.158187933883667, 0.15085136787414552, 0.15431445434093474, 0.14729850753307341, 0.14545999414205552, 0.1425165748167038, 0.13713245714187622, 0.13890226060390473, 0.1385502540397644, 0.13325720913887024, 0.13821604251861572, 0.12657381804466247, 0.12661254945278166, 0.12613130765914918, 0.12391327431678772, 0.12112771456956864, 0.11464779287338257, 0.11181729909896851, 0.11328984543800354, 0.11377034270763398, 0.1008323883986473, 0.10574802324295043, 0.10820005744934082, 0.10481286969661713, 0.09905795003414154, 0.09426881399154663, 0.09879408803462982, 0.0921817758846283, 0.09256136526584625, 0.08773951858997345, 0.08429362442731857, 0.08109971979141235, 0.08877598517417908, 0.07780417762160301, 0.07771572200775147, 0.07332694660663605, 0.07507660377025605, 0.06614410147428512, 0.07138459090709687, 0.07020886647939682, 0.06329490740060806, 0.0699729752111435, 0.05841852659702301, 0.056548518508672714, 0.04863990737080574, 0.05593434973359108, 0.047579533406496045, 0.046168205873966216, 0.04967024815320969, 0.04516664848566055, 0.045978313578367236, 0.042695106117725375, 0.03908742675542831, 0.03815324349403381, 0.04243473481893539, 0.041354452307224274, 0.033228478544354435, 0.02873298491001129, 0.024551974391937254, 0.028498442780971526, 0.026660011978149413, 0.024635316944122316, 0.02255195417881012, 0.02276638911008835, 0.018222047471404077, 0.023342835848331453, 0.02012387459859252, 0.013655122032463551, 0.013202939081490039, 0.01191380793452263, 0.013363014526069165, 0.01023749262958765, 0.010080177546739578, 0.010762641629129648, 0.00915547530323267, 0.00800634430795908, 0.007730902301371098, 0.005661050456166267, 0.005653218875676393, 0.0038660507406294347, 0.0037588649076223373, 0.0032598251727968455, 0.0032806610392406583, 0.0037530811968445777, 0.0028616120625287295, 0.0029218759620189667, 0.0028474428486824034, 0.003111143550798297, 0.0026813673014193773, 0.002364120044335723, 0.002136831684112549, 0.0021484996793419125, 0.002331566419824958, 0.0018038011450320483, 0.0020251487353071572, 0.0018571478560194374, 0.0017830130577087403, 0.0018381078336387872, 0.0017485338308289647, 0.0015822984500974417, 0.0016659856034442783, 0.001678059484064579, 0.0016570671270042657, 0.0018015468500927091, 0.0016612939359620213, 0.0018312588715553284, 0.0015496110907196998, 0.0015026637172698976, 0.0017928305336087941, 0.0016372318458929658, 0.001696693058460951, 0.0017530826332047581, 0.001762792120203376, 0.0018378171444498003, 0.0015298671818524598, 0.0016207730100676416, 0.001641629972755909], 'eval_losses': [3.06428324760437, 1.619687644996643, 1.6784015194320678, 1.216130467185974, 1.2011162525558472, 1.3355233508300781, 1.2213769592666626, 0.7881295136451721, 0.8945823580169677, 1.0148970318222046, 0.8945053635787964, 0.7257067151451111, 0.7634721509552002, 0.615677039604187, 0.9684668829727173, 1.1126605472183229, 0.9849675846099853, 0.776044950466156, 0.647926743927002, 0.6372571904373169, 0.9504205835533142, 1.014908048210144, 0.6121329259109497, 0.7569236064910889, 0.8154392419624329, 0.6869319832801819, 0.8466614199256897, 0.618489725856781, 0.6957943727111816, 0.9230431431961059, 1.1731341189193725, 0.8364108278274536, 0.4956165792274475, 0.5966865528869629, 0.6346965715217591, 0.660125072517395, 0.6044769531440735, 0.9902143346405029, 0.7231530708312989, 0.6679013049125672, 0.619781512928009, 0.594700328130722, 0.5883544596481324, 0.7146290448951721, 0.6144000371170044, 0.6252500655937194, 0.570193137845993, 0.8277587977981568, 0.7914675968551635, 0.5815612034225464, 0.5919339842414856, 0.6504711382293701, 0.6989334425926208, 0.6424439770889282, 0.6441395628547668, 0.44305410911560056, 0.6850143376541138, 0.7884590788078308, 0.5709101343631744, 0.649547462348938, 0.7109746140670776, 0.6330307704830169, 0.7411720955657959, 0.5734762705230713, 0.6502963551521301, 0.5564207949447632, 0.9452783061599731, 0.7011817437553406, 0.5644931290054321, 0.5445949831867218, 0.7063210621261596, 0.5708739803695678, 0.9189916716957093, 0.5976842547512055, 0.6057191285037994, 0.5128701672363282, 0.695650144252777, 0.6090364290046691, 0.5043736017036438, 0.5335776146697998, 0.7398415574264526, 0.5988200138664246, 0.7106185603713989, 0.5985506398963928, 0.5674770043087005, 0.5718149325370788, 0.4886689380645752, 0.44727654747009277, 0.5202628970146179, 0.7291047353744506, 0.5992303870773316, 0.5524752329826355, 0.5610750304603577, 0.6559933232688904, 0.5161755304527282, 0.6664028327560425, 0.5397292210769653, 0.530955923242569, 0.5407446892356873, 0.6262759117317199, 0.5025380057907104, 0.48494760538101195, 0.5237709249401092, 0.5701957113838195, 0.5020439178848266, 0.493828796043396, 0.5448221842765808, 0.5032462781143189, 0.4914497015094757, 0.5078440181541443, 0.5012798342895508, 0.5996997340011597, 0.4243741335105896, 0.5332584999656678, 0.5715864287376404, 0.5103534698677062, 0.5183769536685944, 0.5167785063552857, 0.5477265423679352, 0.5546424225997925, 0.4941183293533325, 0.5064970706939698, 0.48782634976387024, 0.5662731025123596, 0.47978079763412473, 0.5130372922229767, 0.49032960483551025, 0.5251384033966064, 0.5668525117874146, 0.5322475453853607, 0.5279995453453064, 0.5020666910743713, 0.5402168151760102, 0.5011189796447754, 0.4692922214126587, 0.5164243151664734, 0.493190329875946, 0.5785425694274903, 0.47693252155303956, 0.4433224747276306, 0.4668867498207092, 0.5000450556755066, 0.4923539544677734, 0.48933984506607053, 0.43977649684906006, 0.4676870616722107, 0.44429535197257997, 0.4687599349594116, 0.49633583946228027, 0.4681646405124664, 0.4525326292514801, 0.4893134775543213, 0.4667766618824005, 0.4473139002418518, 0.4366284686946869, 0.449866565990448, 0.4351149694919586, 0.43024197170257567, 0.43942650373458864, 0.4453092336893082, 0.4261190866279602, 0.4324530947494507, 0.4197796183395386, 0.4236819329071045, 0.4178492256164551, 0.41815060754776, 0.42385944024086, 0.4149613055992126, 0.4196777898311615, 0.4205210756969452, 0.4220051736354828, 0.4148130286884308, 0.41430414518356323, 0.40838830839633944, 0.4128794789123535, 0.4139006679725647, 0.41162409527778626, 0.40608237585067747, 0.40930646655082703, 0.40990852507591247, 0.4061431446838379, 0.4057943342399597, 0.40607838397026064, 0.409145424118042, 0.41053249230384825, 0.4061631052207947, 0.4074535328578949, 0.40770847448349, 0.4081394024848938, 0.40772172640800475, 0.4082060157585144, 0.409255017080307, 0.4071409948062897, 0.40820804743766786, 0.405010389213562, 0.4109284137535095, 0.40522663500785827, 0.4054895939826965, 0.40735630949020385, 0.4066020632648468, 0.4054249066352844], 'train_acc1es': [32.019999990234375, 51.89999998901367, 61.556, 66.56399998779297, 69.9240000024414, 73.0999999975586, 75.4919999975586, 77.86000000244141, 78.75999998535156, 80.33199997558594, 81.39199999511719, 82.06799997802734, 82.93200000976563, 83.28800000488282, 84.32000000732423, 84.62799999023437, 85.00400001708984, 85.51200001220703, 85.81600000732422, 86.07999999511719, 86.41599997802734, 86.66399999755859, 87.25599997070313, 87.01199999755859, 87.66399999755859, 87.46000000732423, 88.19599999511719, 88.03199998779297, 88.45600000488281, 88.7480000024414, 88.676, 89.148, 89.49599997314454, 89.32799998779296, 89.05599999755859, 89.956, 89.97999998779297, 89.80000000732421, 90.236, 90.37999998535156, 90.60399999267578, 90.58399998779296, 90.63599998535156, 90.54799997802735, 90.62399998535156, 90.91199999267577, 91.32799999511718, 91.39200001708984, 91.19999999023437, 91.31999997070312, 91.55199999267577, 91.85999998779297, 91.73199998291015, 91.77599998779297, 92.12399998535156, 92.23199998779297, 92.18399999511719, 92.14799998535156, 92.59599998779296, 92.33999998535157, 92.85599998291016, 92.82399999267578, 92.73599998779297, 92.98399997558593, 93.02399997558594, 93.14400001708984, 93.23199998291015, 93.35199998046875, 93.57999998046876, 93.22399997802735, 93.61999997558594, 93.51600001953125, 93.75599998291015, 93.83200001464844, 93.89599998046874, 94.11599997070313, 93.98800001953126, 93.89199997558593, 94.25199997802734, 94.19199997314453, 94.29999998535156, 94.37599997802734, 94.75999998779297, 94.58400001708985, 94.81199997558593, 94.69200001220703, 94.90799997558594, 94.90800001708985, 95.09200001708984, 95.31599998535157, 95.18800001464844, 95.16800001708984, 95.37599997558594, 95.30399997558594, 95.52799997070312, 95.65999998046875, 95.6679999975586, 95.57599997314453, 95.80000001464843, 95.97599997558594, 96.17999997314453, 96.04399997314454, 96.07999997558593, 96.65600001953125, 96.37600001464844, 96.18800001953124, 96.43599997070312, 96.62400001220703, 96.73200001708985, 96.63200001220703, 96.73200001464843, 96.87199997070313, 97.00400000976562, 97.02800001464844, 97.26800001953124, 96.92800000244141, 97.38800000732422, 97.37999997314454, 97.64800001953125, 97.39200000732421, 97.78800001464843, 97.53200000732421, 97.78400000976562, 97.96800000976563, 97.66000001220704, 98.03600000488281, 98.10000000488282, 98.40800000732422, 98.0720000024414, 98.47200000732421, 98.49200001708985, 98.36800001464844, 98.55600000732422, 98.46400000244141, 98.70000000488281, 98.7000000024414, 98.72400000976563, 98.60800001220703, 98.67200000732421, 99.02000000488282, 99.16, 99.31200000976563, 99.09200000732422, 99.24000000976562, 99.34000000488281, 99.352, 99.2840000024414, 99.4240000024414, 99.3200000024414, 99.412, 99.6120000024414, 99.62800000244141, 99.68800000244141, 99.632, 99.752, 99.752, 99.732, 99.75600000244141, 99.812, 99.848, 99.90800000244141, 99.884, 99.968, 99.94, 99.972, 99.964, 99.932, 99.968, 99.956, 99.96000000732423, 99.952, 99.952, 99.976, 99.98, 99.98, 99.976, 99.988, 99.988, 99.984, 99.98, 99.984, 99.992, 100.0, 99.996, 99.9840000024414, 99.984, 99.988, 99.992, 99.98800000244141, 99.996, 99.996, 99.988, 99.992, 99.984, 99.984, 99.984, 99.972, 99.996, 99.988, 99.988], 'eval_acc1es': [26.00399999694824, 49.0959999987793, 47.483999992675784, 57.82799998901367, 58.52799998657227, 61.959999989013674, 60.68800000488281, 73.58400000488281, 71.39999999023438, 69.04000001953125, 71.27599998046875, 75.17600001464844, 76.16800001708984, 79.56400001708984, 72.19200001708984, 68.44800001953125, 68.58399998779296, 76.32399999755859, 79.63599997558593, 79.20799997558593, 71.45599997070312, 68.71200001953125, 80.57599997558594, 76.45199999755859, 76.39999998779297, 77.32399999267578, 74.72400000732422, 80.09599997314453, 77.59199998046876, 73.25999998535156, 66.17599998046875, 76.2719999975586, 83.88399997070313, 81.31199998291015, 80.05600001953125, 79.67600001708985, 81.67600001708985, 72.65999999267578, 79.32400000732422, 79.89600001953124, 81.70799998291015, 82.44399999755859, 81.92399998291016, 78.4039999975586, 81.78400000732422, 80.45599997558594, 82.94800000976562, 78.53200001953125, 77.61999998535157, 82.75200001953125, 81.94399997802735, 80.52000000488282, 80.04, 81.35999998779297, 81.81599998535157, 86.276, 80.81599998046875, 78.21599998779297, 83.31999999511719, 81.66400000732422, 80.24400001464844, 81.24000001464844, 79.79999997558593, 83.79200000976563, 81.40399997558593, 83.70399998046875, 76.8199999975586, 81.09999998291016, 83.40000001708984, 84.22800000976562, 81.25599998046874, 83.32400000488282, 76.80399998291016, 83.44400000732422, 83.0040000024414, 85.10000001953125, 80.65199998291016, 82.79599997070312, 84.75600001220702, 84.75600001953126, 80.08399997558594, 82.90799998535157, 81.32800001953125, 83.45199997558593, 83.86399999023438, 83.90399999267578, 85.40400001708984, 86.7720000024414, 85.77199998535156, 81.47999997558594, 83.12799997314453, 85.14399997070312, 84.21200001953125, 82.78000000488281, 85.58800000976562, 82.09200001953126, 85.28800001464843, 85.56399999511719, 85.64399999267579, 83.85599997558593, 86.38399997802735, 86.7239999975586, 85.82400000488282, 85.51200001464844, 86.38800001220703, 86.67600000732422, 85.44399997070313, 85.8999999975586, 86.5359999951172, 86.71200001464844, 86.51200000732422, 85.07599998046875, 88.34399998291016, 86.37599999511718, 85.55200000732422, 86.71999999511719, 86.99199999267579, 86.33200000976562, 86.38799999267579, 86.39200000976562, 87.22000000976563, 87.33200001708984, 87.64399999023438, 85.68000001708984, 87.62799998779298, 87.3280000024414, 87.77999999755859, 87.60400001220704, 86.34400001953125, 86.9480000024414, 87.76, 87.724, 87.15199999023437, 88.06000001708985, 88.532, 88.16399998046874, 88.17199998779297, 86.21999997070313, 88.4000000024414, 89.29199999267578, 88.9919999975586, 88.484, 88.9, 88.43199999023437, 89.56399998291016, 88.9479999975586, 89.8999999975586, 89.39999998779297, 88.59599999267579, 89.34799997558594, 89.65599997802734, 89.092, 89.74799997802734, 90.04399999267578, 90.25999999511718, 90.17999999023438, 90.31199997558593, 90.53199999267578, 90.13199999267579, 90.30799997314453, 90.65199999267578, 90.53599997314453, 90.8280000024414, 90.78800000244141, 90.94800001953125, 91.08799999267578, 90.92399998779297, 90.97599998046876, 91.06799997070313, 90.95599997802735, 90.93199999267578, 91.12399998291015, 91.12799999023437, 91.26399997070313, 91.1279999951172, 91.19199999023438, 91.24799997558594, 91.27999998291016, 91.33199999267578, 91.15999998535156, 91.26799998291015, 91.38399998046874, 91.31999998779297, 91.32799998535157, 91.23199999023437, 91.27999998291016, 91.26399998046875, 91.26799998535157, 91.26399998535156, 91.25999999267579, 91.28399999511718, 91.23199998046876, 91.34399998291016, 91.33599999755859, 91.30800000488281, 91.22399997802735, 91.30399997314453, 91.348, 91.31199998535156, 91.33999998291016, 91.29], 'cost_info': {'flops': 121.81569, 'params': 0.858426, 'latency': 0.021243879669591, 'train_time': 13.872018893559774}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/15 22:07:43 nl.defaults.trainer]: \u001b[0mEpoch 57 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:43 nl.defaults.trainer]: \u001b[0mEpoch 58 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:43 nl.defaults.trainer]: \u001b[0mEpoch 59 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:44 nl.defaults.trainer]: \u001b[0mEpoch 60 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:44 nl.defaults.trainer]: \u001b[0mEpoch 61 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:44 nl.defaults.trainer]: \u001b[0mEpoch 62 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:44 nl.defaults.trainer]: \u001b[0mEpoch 63 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:44 nl.defaults.trainer]: \u001b[0mEpoch 64 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:44 nl.defaults.trainer]: \u001b[0mEpoch 65 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:44 nl.defaults.trainer]: \u001b[0mEpoch 66 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:45 nl.defaults.trainer]: \u001b[0mEpoch 67 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:45 nl.defaults.trainer]: \u001b[0mEpoch 68 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:45 nl.defaults.trainer]: \u001b[0mEpoch 69 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:45 nl.defaults.trainer]: \u001b[0mEpoch 70 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:45 nl.defaults.trainer]: \u001b[0mEpoch 71 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:45 nl.defaults.trainer]: \u001b[0mEpoch 72 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:46 nl.defaults.trainer]: \u001b[0mEpoch 73 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:46 nl.defaults.trainer]: \u001b[0mEpoch 74 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:46 nl.defaults.trainer]: \u001b[0mEpoch 75 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:47 nl.defaults.trainer]: \u001b[0mEpoch 76 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:47 nl.defaults.trainer]: \u001b[0mEpoch 77 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:47 nl.defaults.trainer]: \u001b[0mEpoch 78 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:47 nl.defaults.trainer]: \u001b[0mEpoch 79 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:47 nl.defaults.trainer]: \u001b[0mEpoch 80 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:47 nl.defaults.trainer]: \u001b[0mEpoch 81 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:47 nl.defaults.trainer]: \u001b[0mEpoch 82 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:47 nl.defaults.trainer]: \u001b[0mEpoch 83 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:48 nl.defaults.trainer]: \u001b[0mEpoch 84 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:48 nl.defaults.trainer]: \u001b[0mEpoch 85 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:48 nl.defaults.trainer]: \u001b[0mEpoch 86 done. Train accuracy (top1, top5): 99.98800, 0.00000, Validation accuracy: 91.29000, 0.00000\n",
      "\u001b[32m[11/15 22:07:48 nl.defaults.trainer]: \u001b[0mEpoch 87 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:48 nl.defaults.trainer]: \u001b[0mEpoch 88 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:48 nl.defaults.trainer]: \u001b[0mEpoch 89 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:48 nl.defaults.trainer]: \u001b[0mEpoch 90, Anytime results: {'cifar10-valid': {'train_losses': [1.7566959915542602, 1.2562305207061768, 1.021969607105255, 0.887471586341858, 0.7711264366722107, 0.6973593336296081, 0.6358724685668945, 0.5887547205162048, 0.5553631452560425, 0.5191056136131287, 0.5053842701339721, 0.48410842166900636, 0.4608239453411102, 0.44615772003173826, 0.42765160623550413, 0.4169538708496094, 0.4113772527885437, 0.400520688533783, 0.3873916826248169, 0.3724646677875519, 0.3693948192501068, 0.3559830383682251, 0.3505366324520111, 0.3452281665992737, 0.34062314281463624, 0.32785758745193483, 0.3247086066532135, 0.3152397169876099, 0.3088342436790466, 0.30932147022247314, 0.30630003730773925, 0.2997372068309784, 0.2971782715511322, 0.28651712685585023, 0.2870645102787018, 0.27504950336933137, 0.2777867415809631, 0.26797383105278016, 0.26828141214370727, 0.2608852974796295, 0.26271526859283445, 0.25819563908576965, 0.25635491406440736, 0.24965522625923156, 0.2520431183052063, 0.2375393706226349, 0.23917241463661193, 0.23801521068572998, 0.23188298479557037, 0.2331004785490036, 0.23114698135375977, 0.23154076245307922, 0.2203089067173004, 0.22347980970382691, 0.21881041046142577, 0.21870132612228393, 0.21887179215431213, 0.20907892333984374, 0.20332559474468231, 0.2089519344997406, 0.1995320409822464, 0.19729788772583007, 0.20318201966285707, 0.19733836444854735, 0.19641056103229523, 0.18655153985023498, 0.19506199403285981, 0.1799078153371811, 0.1785960203075409, 0.18561954021453858, 0.17252663808345794, 0.17490434623241424, 0.18034411867141723, 0.1770526053571701, 0.15727929019927978, 0.16457584752559662, 0.16350530787467957, 0.1744671707010269, 0.15580591336250305, 0.16035158840179442, 0.16160971426010132, 0.15041578160762786, 0.1530300564289093, 0.14840648898124695, 0.14594281682014465, 0.14496576875686645, 0.13973895180225374, 0.1505095663022995, 0.12859554119586944, 0.1333027033996582, 0.13799145790576936, 0.12559545964956284, 0.12823138360977174, 0.13122628460407257, 0.11851615238666534, 0.11987431374073028, 0.11895439136505127, 0.11576267394542694, 0.10813020124435425, 0.11560182755470276, 0.11379308485269546, 0.10941141399145127, 0.10136059791564941, 0.10484065237998963, 0.10627953226566314, 0.10193422690153121, 0.09161850289344788, 0.10427542674064637, 0.09334836124897003, 0.09348245196819305, 0.08534761848211289, 0.08061939785718918, 0.08767773209571839, 0.08878641590356827, 0.08138755255579948, 0.0750544572854042, 0.07969755893945694, 0.06847826910257339, 0.0669090148472786, 0.07137500919461251, 0.06921156039714814, 0.06502463127851486, 0.06775549052953721, 0.06403918340682983, 0.060477198193073275, 0.05546410901784897, 0.05496094081282615, 0.05736133122205734, 0.05577936714887619, 0.04899605953812599, 0.04792055790662766, 0.03839496186852455, 0.04373454230546951, 0.042885961420536044, 0.03593592415094376, 0.038482503937482836, 0.03917034383058548, 0.03687569484233856, 0.032553034116029736, 0.02801773777782917, 0.03402911237001419, 0.029614294304847716, 0.02716249799370766, 0.02417993843615055, 0.02280869387626648, 0.018543447163403035, 0.019287542381584644, 0.017245020906329154, 0.019535486179590226, 0.019757490668296813, 0.017012828702181577, 0.0179872754868865, 0.014241337795257568, 0.012959931439608335, 0.010569193668961524, 0.011022652215957641, 0.009986528376936912, 0.007922794835269451, 0.008557118722200394, 0.007718300522714853, 0.0075397859817743305, 0.004741646375507116, 0.005334809516072273, 0.005128208494558931, 0.004326937962025404, 0.004133038453310728, 0.0027530523860454558, 0.003621739902533591, 0.0035839379880577326, 0.0035357081224769355, 0.003933330335319042, 0.0030383005800843237, 0.0029338506411388516, 0.0024989218328148126, 0.002257402706220746, 0.0023273499113321302, 0.0022183405302092433, 0.0015654075241833924, 0.001697110738568008, 0.001538765897527337, 0.0017889395431429148, 0.0018439432050287724, 0.0014448394963145256, 0.0015098131747543811, 0.0014834808644652367, 0.001448985576890409, 0.001502575025446713, 0.001466497707143426, 0.0014927434063516558, 0.001553286456912756, 0.0014569178768247366, 0.0013568095971271397, 0.0014476653285324574, 0.0013142505932971836, 0.0014138608738034964, 0.0016697864532284439, 0.0014324690724536776, 0.0014736034300550818, 0.0013286740584671497, 0.0016212545205652714], 'eval_losses': [2.0293397730255127, 1.5501235927581787, 1.181889102935791, 1.3462502548217774, 1.2875802408599855, 0.9919574428176879, 0.8829674390792847, 0.9942334904098511, 0.7778971092796325, 0.9525061672210693, 0.7610175980377197, 0.8091353378677368, 0.6445176959133149, 0.7908354474639893, 0.6356363484954834, 0.6613211820030213, 0.6924763288879394, 0.6395409346199036, 0.637983805809021, 0.8102348127365112, 0.5660454766273498, 0.917141348361969, 1.0677683911895752, 0.5980622875785828, 0.7158046905708313, 0.5929009234046936, 0.6144597950553894, 0.7353570256042481, 0.6189310663604737, 0.9336430278968811, 0.6908340557098389, 0.7031524319648743, 0.514897764596939, 0.7753351585006714, 0.6091403286743164, 0.5330676745223999, 0.8611484060287475, 0.5893562986373901, 0.5549210845375061, 0.5208159957504273, 0.7188507621765137, 0.5001718541145325, 0.683203807849884, 0.5524386884212494, 0.5728103044509888, 0.6242404249763489, 0.6490500361633301, 0.7631304513931274, 0.6282349925041198, 0.6486051977729798, 0.6188981666564941, 0.7863550980949402, 0.7486588953018188, 0.7456559461593628, 0.57287781457901, 0.4834386502170563, 0.6167054035949707, 0.5920968911743164, 0.5410513415908813, 0.5312601464748382, 0.7085112562179565, 0.6162702812004089, 1.106122700214386, 0.6750489998054504, 0.6373956297683716, 0.5385980323028564, 0.5298845402145386, 0.517370371723175, 0.6203322114753723, 0.6951945735549927, 0.5641009483337402, 0.6040038746452332, 0.5766871203041076, 0.5106198987770081, 0.5712802279281616, 0.5625830656814576, 0.5371810247707367, 0.5611917540359497, 0.5877152471351623, 0.5601113037395478, 0.4781811703681946, 0.5701240014743805, 0.5141382973575592, 0.4747303805732727, 0.6430161083984375, 0.7803051251602173, 0.6217226250076294, 0.4981024032402039, 0.5145438384914398, 0.5515872966384888, 0.5747680339622497, 0.4993633393192291, 0.5645359240341187, 0.541810246219635, 0.5093781566905975, 0.5495955912399292, 0.5213512135505676, 0.493306043176651, 0.5257548937988281, 0.6347305177307129, 0.4727744797706604, 0.5535139004516602, 0.5241530282306671, 0.5965076839256287, 0.5495537710475922, 0.46518791153907774, 0.5331688012981415, 0.46819933597564695, 0.5629283952903748, 0.5409698378372192, 0.5338248561096192, 0.4745099057674408, 0.5721999285316467, 0.4530285828590393, 0.5094747659015656, 0.5084494667720795, 0.5691023572349548, 0.49519070366859436, 0.5429869326114655, 0.4732337914466858, 0.4742864581298828, 0.5931675081825256, 0.46185193899154664, 0.5196695386314392, 0.4808735268688202, 0.5621578010177612, 0.4981731525993347, 0.529577971572876, 0.5960051606750488, 0.48004251918792723, 0.5003827950572968, 0.508306653213501, 0.46552496728897097, 0.4745150417137146, 0.5312333197593689, 0.5677153014183044, 0.5316101592826843, 0.47052358480453493, 0.47479769664764404, 0.5582830512237549, 0.5169115965080261, 0.4643036765384674, 0.4481639535522461, 0.4676883477020264, 0.43379057567596435, 0.47160497773170473, 0.4912052928066254, 0.5208686095809937, 0.48558499448776243, 0.44935792890548704, 0.4664529674911499, 0.46782843439102173, 0.45881551748275756, 0.45668567426681517, 0.45580918785095215, 0.47979998788833617, 0.46212193029403686, 0.4659665923643112, 0.4615324252986908, 0.4698526006221771, 0.4446336248970032, 0.4461898690986633, 0.4636729027080536, 0.45219546699523927, 0.45085052276611326, 0.45250487379074095, 0.44030157356262206, 0.446105463476181, 0.44120370779037477, 0.43593546913146974, 0.43812386402130127, 0.4404348994350433, 0.4362802892303467, 0.4418835751914978, 0.4350903349781036, 0.4366686417293549, 0.4380669884586334, 0.43474117760658265, 0.4342274220371246, 0.43069450110435487, 0.43578112520217893, 0.4381164241218567, 0.4393138302898407, 0.4329405191612244, 0.43340056483268735, 0.43374170457839967, 0.43400984152793887, 0.4345460909080505, 0.4330478945064545, 0.43479715286254883, 0.4368761696815491, 0.43278942407608034, 0.4312318443107605, 0.4305556021690369, 0.43574370643615723, 0.43381337968826295, 0.434352371673584, 0.43424666302204135, 0.43303508944511415, 0.433920128364563, 0.44216036639213563], 'train_acc1es': [32.92000000732422, 53.72400000244141, 63.259999978027345, 68.25999999023438, 72.52799997802734, 75.52799999511718, 77.78399998046875, 79.31999997558594, 80.62800001953126, 81.96399997314452, 82.54799998535157, 83.24000000732421, 84.17999997070312, 84.61600001464844, 85.06799998535156, 85.63999997558594, 85.69199997314453, 86.00400001220703, 86.46399998046876, 87.06799998535156, 87.1240000024414, 87.67600000976563, 87.72000000244141, 88.13199999511718, 88.3759999975586, 88.68799998046875, 88.63600000732421, 89.06, 89.4560000024414, 89.2040000024414, 89.26399999023438, 89.36000001220702, 89.73600000732422, 89.956, 89.96399999511719, 90.45199998291015, 90.46799999267579, 90.82399998535156, 90.85999998779297, 91.04000001464844, 90.81199997802734, 91.03199998046875, 91.14399998535156, 91.37599999267579, 91.23599998291016, 91.81599999755859, 91.84399999511719, 91.5239999951172, 92.05599998535156, 91.99599997314454, 91.81199998779297, 92.07999997558593, 92.37599998291016, 92.21199999755859, 92.30799997070312, 92.51200001953126, 92.45199997802735, 92.86799997314453, 92.96799997802735, 92.74799997802734, 93.13599997314454, 93.18399998046876, 93.0399999975586, 93.23599997070312, 93.09199998535156, 93.44000001220704, 93.39200001953125, 93.66799998046875, 93.65599998046875, 93.50799998535156, 94.06799997802734, 94.06399997314453, 93.85199998046875, 93.81999998046875, 94.59199997558594, 94.30400001220703, 94.33999997558594, 93.91199997802734, 94.62399998535156, 94.41599997558593, 94.42000001464844, 94.84399997802734, 94.70399998535156, 94.86399997070312, 94.87600001953125, 94.94400001953125, 95.13599997802734, 94.89599998046874, 95.59199997802735, 95.36399997070312, 95.22400001220703, 95.54000001464844, 95.66000001708984, 95.46399997314452, 95.81199997070313, 95.82800001953125, 95.98399997070312, 96.08000000732422, 96.21200001708985, 95.88800001464844, 96.06400001220703, 96.28400001708984, 96.59600001464844, 96.46799998291016, 96.44400000976563, 96.52800001464844, 96.87200001708985, 96.40800001708985, 96.95200001464843, 96.73600001708985, 97.10000000976562, 97.35600001220703, 96.95599997314453, 97.05200001220703, 97.15200000732422, 97.39200000976562, 97.17999997314453, 97.68800001220703, 97.64400001220703, 97.65600000732422, 97.64000001708985, 97.77600000976562, 97.72000000732422, 97.83199997070312, 97.89200000732421, 98.13200001708984, 98.212, 98.17200001220704, 98.12400000488282, 98.4640000048828, 98.44000000488282, 98.79200000244141, 98.58000000976563, 98.56400000976562, 98.80000001220704, 98.7320000048828, 98.68800000244141, 98.84800000732422, 99.04800000976563, 99.128, 98.92800000488282, 99.08800000976562, 99.18400000244141, 99.2280000024414, 99.29600000488281, 99.456, 99.468, 99.4800000024414, 99.42400000488281, 99.4120000024414, 99.488, 99.4480000024414, 99.584, 99.652, 99.72400000488281, 99.7, 99.716, 99.78800000244141, 99.78, 99.82, 99.80800000488281, 99.924, 99.9000000024414, 99.876, 99.908, 99.88400000244141, 99.976, 99.924, 99.94, 99.932, 99.91600000244141, 99.95200000244141, 99.948, 99.968, 99.972, 99.972, 99.98, 99.992, 99.992, 99.996, 99.972, 99.988, 99.996, 99.988, 99.98800000244141, 99.996, 99.996, 99.988, 99.984, 99.988, 99.988, 99.996, 99.992, 99.996, 99.996, 99.98, 99.992, 99.992, 100.0, 99.984], 'eval_acc1es': [29.559999986572265, 51.647999993896484, 59.875999991455075, 55.71999998779297, 61.24799999145508, 67.64399999023438, 70.1199999975586, 69.21599997558594, 74.38400001464844, 72.36799997802734, 74.85999999755859, 74.20399998291016, 79.00000001220702, 74.52800000976562, 78.82399999511719, 78.20399998291016, 78.03999998535156, 79.79599998291016, 79.892, 75.4040000024414, 81.3640000024414, 74.12799999755859, 71.13600000488282, 80.40000000732422, 78.15200001708985, 80.84800000732422, 81.07199997314453, 78.55600001220704, 80.30799998291016, 73.74800000488281, 78.99999999755859, 78.41599997070313, 83.73600001464844, 78.37999997802734, 82.01200001953126, 83.22400000976563, 76.252, 82.30399998291016, 82.83200000488281, 84.02400001464844, 78.52400000244141, 84.52800001708984, 80.16399997070313, 83.48800001220702, 82.23599997802734, 80.88800000732422, 80.65599997070312, 79.17200001953125, 81.87599997802734, 81.10399997802735, 82.54000000976562, 79.51199998046874, 78.1319999975586, 79.46000000976562, 82.91600000976563, 85.29200000244141, 82.65999997070313, 82.12800000488281, 84.4040000024414, 84.5320000024414, 81.29200001220703, 82.43999997558593, 73.91199998291016, 81.41999997314453, 81.46800001708985, 84.14400001220703, 83.80399997558594, 84.72399999511718, 82.27599997070313, 81.67599997314453, 83.94000001464843, 82.52399998046874, 83.59199997314452, 85.33600001464843, 84.3999999975586, 83.79599997070312, 84.69199997070312, 84.05200000732422, 83.96400001708984, 84.14800001220704, 86.10800001708985, 84.01199999267578, 85.06000000244141, 86.22800000488282, 82.988, 80.25199998779297, 83.36799997802734, 85.89200001708984, 85.68800000244141, 84.8880000024414, 84.30000001464843, 86.1519999951172, 84.61999998535157, 84.99599997314454, 85.8640000024414, 85.22400000732422, 86.15200001708985, 86.2960000024414, 86.0960000024414, 82.81200000976563, 86.94399998535157, 84.83200000976562, 86.19599999511719, 84.56800001464843, 85.41599999267578, 87.30799998779297, 85.89200000488282, 87.07200000976563, 85.61199998535156, 85.96800000488281, 86.06400001708984, 87.49199998779297, 85.9000000024414, 87.85199999023438, 87.07999998779297, 86.31200000244141, 85.43199999511718, 86.7559999951172, 86.83999999511718, 87.89200000488282, 87.68400000244141, 85.91200001464844, 87.89999998291016, 87.20000000976563, 88.2240000024414, 86.3999999975586, 87.63199999023438, 86.68399999755859, 86.13599997314454, 87.53600000244141, 88.0359999951172, 87.8800000024414, 88.584, 88.57199999267579, 87.528, 86.79999997558593, 87.60399998535156, 89.07200001220703, 88.648, 87.692, 88.30400001708985, 89.22399998779296, 89.50800000732421, 89.108, 89.64400001220703, 89.46399999023437, 89.22, 88.31199999511719, 89.35599999023438, 89.77199999511718, 89.75999998535156, 89.68399998535156, 89.72399999023438, 89.86399997558594, 90.08799999267578, 89.98799998535156, 89.69199998779297, 90.17999997070312, 90.31999999023438, 90.08799998046875, 90.39199999023438, 90.47999997314453, 90.23599998291016, 90.46399999023437, 90.60399998779297, 90.61599999755859, 91.04799998046875, 90.61599998535156, 90.86399999511718, 90.96799998779296, 90.86399999267579, 90.80399998535157, 90.87199997802735, 90.8879999975586, 91.10800001708985, 90.99199997558594, 91.17199998535156, 91.07999998535156, 91.07599997070312, 91.13999998535157, 91.08399998779296, 91.09600000976563, 91.10799998046875, 91.15999999267578, 91.09599998291016, 91.18399997314454, 91.18399997314454, 91.13199998291016, 91.17999997070312, 91.13199997558594, 91.15199998535157, 91.17199997314454, 91.16400001953124, 91.15599998779297, 91.20399999023438, 91.16399999023437, 91.13599998291015, 91.22800001708984, 91.09199998046876, 91.15199998779296, 91.31], 'cost_info': {'flops': 149.34081, 'params': 1.045466, 'latency': 0.019136852339694373, 'train_time': 12.285543183485666}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/15 22:07:48 nl.defaults.trainer]: \u001b[0mEpoch 90 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:48 nl.defaults.trainer]: \u001b[0mEpoch 91 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:50 nl.defaults.trainer]: \u001b[0mEpoch 92 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:50 nl.defaults.trainer]: \u001b[0mEpoch 93 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:50 nl.defaults.trainer]: \u001b[0mEpoch 94 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:50 nl.defaults.trainer]: \u001b[0mEpoch 95 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:50 nl.defaults.trainer]: \u001b[0mEpoch 96 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:50 nl.defaults.trainer]: \u001b[0mEpoch 97 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:50 nl.defaults.trainer]: \u001b[0mEpoch 98 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:51 nl.defaults.trainer]: \u001b[0mEpoch 99 done. Train accuracy (top1, top5): 99.98400, 0.00000, Validation accuracy: 91.31000, 0.00000\n",
      "\u001b[32m[11/15 22:07:51 nl.defaults.trainer]: \u001b[0mTraining finished\n"
     ]
    }
   ],
   "source": [
    "# call only a method to run the search for the number of iterations specified in the yaml configuration file.\n",
    "trainer.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/15 22:07:58 nl.defaults.trainer]: \u001b[0mStart evaluation\n",
      "\u001b[32m[11/15 22:07:58 nl.defaults.trainer]: \u001b[0mloading model from file run/cifar10/nas_predictors/nasbench201/var_sparse_gp/1000/search/model_final.pth\n",
      "\u001b[32m[11/15 22:07:58 nl.defaults.trainer]: \u001b[0mFinal architecture:\n",
      "Graph makrograph-0.4395186, scope None, 20 nodes\n",
      "\u001b[32m[11/15 22:07:58 nl.defaults.trainer]: \u001b[0mQueried results (Metric.TEST_ACCURACY): 91.31\n"
     ]
    }
   ],
   "source": [
    "# After the search is done, we want to evaluate the test performance of\n",
    "# the best architecture found using the validation set.\n",
    "trainer.evaluate(dataset_api=dataset_api)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
