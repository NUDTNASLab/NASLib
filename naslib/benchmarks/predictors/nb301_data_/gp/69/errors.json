[{"experiment_type":"vary_train_size","search_space":"darts","dataset":"cifar10","predictor":"gp","uniform_random":1,"test_size":200,"train_size_single":10,"train_size_list":[5,8,12,20,31,50,79,126,199,316,500],"fidelity_single":5,"fidelity_list":[1,2,3,4,5,7,10,13,17,23,30,40,54,71,95],"out_dir":"p301_60","max_hpo_time":900,"seed":69,"search":{"seed":1000,"batch_size":256,"data_size":25000,"cutout":false,"cutout_length":16,"cutout_prob":1.0,"train_portion":0.7},"eval_only":false,"resume":false,"model_path":null,"save":"p301_60/cifar10/predictors/gp/69","data":"/home/shalag/NASLib/naslib/data"},{"mae":0.5523199615478532,"rmse":0.6833043538130762,"pearson":NaN,"spearman":NaN,"kendalltau":NaN,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.0,"precision_20":0.0,"train_size":5,"fidelity":5,"train_time":38781.07628,"fit_time":0.0048558712005615234,"query_time":5.147695541381836e-05,"cv_score":0},{"mae":0.6165000915527343,"rmse":0.745874595659039,"pearson":NaN,"spearman":NaN,"kendalltau":NaN,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.0,"precision_20":0.0,"train_size":8,"fidelity":5,"train_time":63653.159477,"fit_time":0.0024459362030029297,"query_time":4.9557685852050785e-05,"cv_score":0},{"mae":0.5882335408528665,"rmse":0.7177188209529436,"pearson":6.075896110383734e-15,"spearman":NaN,"kendalltau":NaN,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.0,"precision_20":0.0,"train_size":12,"fidelity":5,"train_time":93226.110098,"fit_time":0.0023958683013916016,"query_time":4.986405372619629e-05,"cv_score":0},{"mae":0.5132315065126083,"rmse":0.6495310321514545,"pearson":0.22558462480297065,"spearman":0.1966606802004531,"kendalltau":0.13097945948119996,"kt_2dec":0.13249862631131187,"kt_1dec":0.171352521197016,"precision_10":0.1,"precision_20":0.2,"train_size":20,"fidelity":5,"train_time":160622.902475,"fit_time":0.002844572067260742,"query_time":5.1017999649047855e-05,"cv_score":0},{"mae":0.5236182343935958,"rmse":0.6601020483584066,"pearson":0.09594837059817156,"spearman":0.12276900831522403,"kendalltau":0.0857411692450009,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.0,"precision_20":0.25,"train_size":31,"fidelity":5,"train_time":246560.976459,"fit_time":0.003378629684448242,"query_time":5.111098289489746e-05,"cv_score":0},{"mae":0.5216401412962847,"rmse":0.6589454234920236,"pearson":0.08870484879401255,"spearman":0.057980868051860826,"kendalltau":0.04704130747641221,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.0,"precision_20":0.1,"train_size":50,"fidelity":5,"train_time":393619.77007400006,"fit_time":0.016257286071777344,"query_time":8.117198944091796e-05,"cv_score":0},{"mae":0.5305279695535012,"rmse":0.6645582020839151,"pearson":6.075896110383734e-15,"spearman":NaN,"kendalltau":NaN,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.0,"precision_20":0.0,"train_size":79,"fidelity":5,"train_time":625606.6293169999,"fit_time":0.009917974472045898,"query_time":5.6945085525512695e-05,"cv_score":0},{"mae":0.5241933396250488,"rmse":0.660527199235935,"pearson":0.024571359141244878,"spearman":0.04174834681396451,"kendalltau":0.03425530560734249,"kt_2dec":0.03425530560734249,"kt_1dec":0.03425530560734249,"precision_10":0.0,"precision_20":0.0,"train_size":126,"fidelity":5,"train_time":999047.86418,"fit_time":0.008393049240112305,"query_time":5.6749582290649415e-05,"cv_score":0},{"mae":0.5029263243079456,"rmse":0.6422305190165416,"pearson":0.3792218592309861,"spearman":0.40976181673803597,"kendalltau":0.2772734403786633,"kt_2dec":0.28262806600615636,"kt_1dec":0.2752199382444378,"precision_10":0.2,"precision_20":0.2,"train_size":199,"fidelity":5,"train_time":1573822.860258,"fit_time":0.01207113265991211,"query_time":5.6709051132202146e-05,"cv_score":0},{"mae":0.5100620188494538,"rmse":0.6529524529583656,"pearson":0.09604686768242146,"spearman":0.10126896825558537,"kendalltau":0.08292887426037418,"kt_2dec":0.08292887426037418,"kt_1dec":0.08292887426037418,"precision_10":0.0,"precision_20":0.0,"train_size":316,"fidelity":5,"train_time":2505704.347461,"fit_time":0.0206756591796875,"query_time":6.834864616394043e-05,"cv_score":0},{"mae":0.5117280863521492,"rmse":0.6534479972549223,"pearson":0.0959271535124206,"spearman":0.22521169223856713,"kendalltau":0.18061511099160937,"kt_2dec":0.08292887426037418,"kt_1dec":0.08292887426037418,"precision_10":0.4,"precision_20":0.3,"train_size":500,"fidelity":5,"train_time":3963063.8709429996,"fit_time":0.035474538803100586,"query_time":8.149981498718261e-05,"cv_score":0}]