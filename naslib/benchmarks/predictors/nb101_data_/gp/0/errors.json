[{"experiment_type":"vary_train_size","search_space":"nasbench101","dataset":"cifar10","predictor":"gp","uniform_random":1,"test_size":200,"train_size_single":10,"train_size_list":[5,8,14,24,42,71,121,205,347,589,1000],"fidelity_single":5,"fidelity_list":[1,2,3,4,5,7,10,14,18,25,33,44,60,80,106],"out_dir":"p101_0","max_hpo_time":900,"seed":0,"search":{"seed":1000,"batch_size":256,"data_size":25000,"cutout":false,"cutout_length":16,"cutout_prob":1.0,"train_portion":0.7},"eval_only":false,"resume":false,"model_path":null,"save":"p101_0/cifar10/predictors/gp/0","data":"/home/shalag/NASLib/naslib/data"},{"mae":3.1826124698631197,"rmse":4.103720760494095,"pearson":0.27995068454251815,"spearman":0.26159388620345425,"kendalltau":0.17774662251604276,"kt_2dec":0.1779935658058157,"kt_1dec":0.1787195512695957,"precision_10":0.0,"precision_20":0.1,"train_size":5,"fidelity":5,"train_time":5301.927001953125,"fit_time":0.00516510009765625,"query_time":3.30662727355957e-05,"cv_score":0},{"mae":3.2298927158117294,"rmse":4.068144172236516,"pearson":NaN,"spearman":NaN,"kendalltau":NaN,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.0,"precision_20":0.0,"train_size":8,"fidelity":5,"train_time":8283.864074707031,"fit_time":0.0019445419311523438,"query_time":2.4999380111694337e-05,"cv_score":0},{"mae":3.235512350285875,"rmse":4.047426203873779,"pearson":0.05708597673939869,"spearman":0.07760119556359171,"kendalltau":0.06308909138935173,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.0,"precision_20":0.0,"train_size":14,"fidelity":5,"train_time":13675.190002441406,"fit_time":0.0021004676818847656,"query_time":2.8499364852905273e-05,"cv_score":0},{"mae":3.291094359701114,"rmse":3.9579674398735225,"pearson":0.24937079136688428,"spearman":0.2522897854153298,"kendalltau":0.17017009065673636,"kt_2dec":0.17063843802164486,"kt_1dec":0.1823188056026376,"precision_10":0.0,"precision_20":0.05,"train_size":24,"fidelity":5,"train_time":23070.024047851562,"fit_time":0.002234220504760742,"query_time":2.5016069412231445e-05,"cv_score":0},{"mae":3.2936932978800018,"rmse":4.018186292778568,"pearson":0.0743372218152052,"spearman":0.2166607631702751,"kendalltau":0.14292673193908512,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.0,"precision_20":0.1,"train_size":42,"fidelity":5,"train_time":49452.576110839844,"fit_time":0.011897563934326172,"query_time":2.5649070739746093e-05,"cv_score":0},{"mae":3.284686902456345,"rmse":4.0177748770615995,"pearson":0.11748562699139149,"spearman":0.2678459288892892,"kendalltau":0.1820764474296358,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.0,"precision_20":0.2,"train_size":71,"fidelity":5,"train_time":84433.98843383789,"fit_time":0.021221637725830078,"query_time":6.37662410736084e-05,"cv_score":0},{"mae":3.2565693794067854,"rmse":4.024403545102733,"pearson":0.02396876134571424,"spearman":0.2723289736673988,"kendalltau":0.18736098080038732,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.0,"precision_20":0.1,"train_size":121,"fidelity":5,"train_time":142428.02883911133,"fit_time":0.004706382751464844,"query_time":2.8164386749267577e-05,"cv_score":0},{"mae":3.235490994044378,"rmse":4.047469223111735,"pearson":0.0006349368930391088,"spearman":0.18554936866383773,"kendalltau":0.14755432719448258,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.1,"precision_20":0.1,"train_size":205,"fidelity":5,"train_time":260308.19552612305,"fit_time":0.007108449935913086,"query_time":3.0401945114135742e-05,"cv_score":0},{"mae":3.2717799910931427,"rmse":4.018944276243891,"pearson":0.05367093825767017,"spearman":0.3575324934440794,"kendalltau":0.24332355388005913,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.2,"precision_20":0.15,"train_size":347,"fidelity":5,"train_time":431072.8264770508,"fit_time":0.012436628341674805,"query_time":3.833889961242676e-05,"cv_score":0},{"mae":3.2486762214751734,"rmse":4.0302245784196895,"pearson":0.06250706703360867,"spearman":0.3695213722856776,"kendalltau":0.2533696022245419,"kt_2dec":NaN,"kt_1dec":NaN,"precision_10":0.1,"precision_20":0.2,"train_size":589,"fidelity":5,"train_time":726343.6463928223,"fit_time":0.02689361572265625,"query_time":6.002664566040039e-05,"cv_score":0},{"mae":2.3894687433427033,"rmse":3.0843270092425152,"pearson":0.6514463713696984,"spearman":0.6785395190891965,"kendalltau":0.48689936108157794,"kt_2dec":0.4873569380107587,"kt_1dec":0.49050374249599615,"precision_10":0.2,"precision_20":0.35,"train_size":1000,"fidelity":5,"train_time":1214969.5267028809,"fit_time":0.06621170043945312,"query_time":0.00011370658874511719,"cv_score":0}]